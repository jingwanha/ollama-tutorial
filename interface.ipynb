{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Interface 호출방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here to assist you in any way I can. If you have a question or need information, please go ahead and ask. I strive to provide the best possible answers while ensuring they are safe, positive, and free from harmful or unethical content. Let me know how I can be of help!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "model_name = 'eeve'\n",
    "client = OpenAI(base_url='http://localhost:11455/v1',\n",
    "                api_key=\"ollama\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages = [{\"role\" : \"system\", \"content\" : \"Yor are a helpful assistant.\"},\n",
    "        {\"role\" : \"user\" , \"content\" : \"hi\"}\n",
    "        ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain - ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello there! I'm here to assist you with any questions or topics that you may have. Please feel free to ask, and I will do my best to provide helpful and accurate information in a safe and respectful manner. If a question does not make sense or cannot be answered safely, I will explain why instead of providing false or misleading information. Let's get started!\", response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 0, 'total_tokens': 79}, 'model_name': 'eeve', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-c5c4c863-c1d6-47f2-af6a-f858bb0ceda3-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_name = \"eeve\"\n",
    "llm = ChatOpenAI(model=model_name,\n",
    "                 base_url=\"http://localhost:11455/v1\",\n",
    "                 api_key=\"ollama\")\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm here to help you with any questions or concerns you may have. Please feel free to ask me anything, and I'll do my best to provide a helpful and informative response. If I don't know the answer to your question, I'll let you know and suggest where you might be able to find the information you're looking for. Is there something specific you would like to discuss?\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model_name = 'eeve'\n",
    "llm = Ollama(model=model_name,\n",
    "             base_url=\"http://localhost:11455\")\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm here to help you with any questions or concerns you may have. Please feel free to ask me anything, and I'll do my best to provide a helpful and informative response. If I don't know the answer to your question, I'll let you know and suggest ways we can find the information together. Let's get started!\", response_metadata={'model': 'eeve', 'created_at': '2024-06-21T09:01:41.742554381Z', 'message': {'role': 'assistant', 'content': ''}, 'done': True, 'total_duration': 2052549252, 'load_duration': 713355, 'prompt_eval_duration': 59693000, 'eval_count': 77, 'eval_duration': 1990308000}, id='run-01ae9ab6-d7ab-49ee-9288-281339f96165-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model_name = 'eeve'\n",
    "llm = ChatOllama(model=model_name,\n",
    "             base_url=\"http://localhost:11455\")\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OllamaFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model_name = 'eeve'\n",
    "llm = OllamaFunctions(model=model_name,\n",
    "                      base_url = 'http://localhost:11455')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
